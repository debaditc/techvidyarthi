+++ 
draft = true
date = 2020-07-05T01:35:10-04:00
title = "Trees in Machine Learning World"
description = ""
slug = "" 
tags = []
categories = []
externalLink = ""
series = []
+++

Trees are well-known as a non-linear data structure. They don’t store data in a linear way. They organize data hierarchically.

A tree is a collection of entities called nodes. Nodes are connected by edges. Each node contains a value or data, and it may or may not have a child node.

The Image at the beginning of the article (Source) explains the terminologies of a Tree.

# Decision Trees
Decision trees represent rules, which can be understood by humans and used in knowledge system such as database

{{< figure src="/images/decisiontrees.jpg" >}}
{{< figure src="/images/decisiontrees2.jpg" >}}
Here is the [Link](https://www.youtube.com/watch?v=eKD5gxPPeY0) and source of above image

# Strengths
* Less computation process is required
* Rules generated by Decision trees are quite easy to understand

# Weakness
* Perform poorly with many class and small data.
* Training is computationally expensive process . As at each node, each field (which is getting splitted) must be sorted before its best split can be found.
* Its not the best choice for predicting continuous attributes
* Random Forrest
* Learning ensemble consisting of a bagging of un-pruned decision tree learners with a randomized selection of features at each split

# Purpose (in nutshell)
* Definition : Collection of unpruned CARTs + Rule to combine individual tree decisions
* Purpose : Improve prediction accuracy
* Principle : Encouraging diversity among the tree
* Solution : Bagging + Random decision trees (rCART)

# Flow
* Let N trees be the number of trees to build for each of N trees iterations
* Select a new bootstrap sample (See Bagging) from training set
* Grow an un-pruned tree on this bootstrap.
* At each internal node, randomly select ‘m’ predictors and determine the best split using only these predictors.
* Do not perform cost complexity pruning. Save tree as is, along side those built thus far.
* Output overall prediction as the average response (regression) or majority vote (classification) from all individually trained trees

# Strengths
* Unlike decision trees - it can handle thousands of input variables without variable deletion.
* Forrest generated during the process can be used for other tasks
* With my personal experience , it runs efficiently on large datasets

# Weakness
* It has been observed to overfit for some datasets with noisy classification/regression tasks.
* The categorical variables with different number of levels, the algorithm is biased in favor of those attributes with more levels.

# My DataBricks Notebook
I am publishing my Databricks Notebook where I have tested the Decision Tree and Random Forrest Spark code (which is referred from Apache Spark site)

# Decision Trees
[Link for Databricks - Decision Trees](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2718522690254083/2090336460746042/4283590658906401/latest.html)


# Random Forrest
[Link for Databricks - Random forrest](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/2718522690254083/1766630042158090/4283590658906401/latest.html)

# Note
This article is prepared after reading a lot of articles from various sources (blogs,pdfs,lectures,videos,etc) and articulating them in single blog.

For this article, we will keep it short with Decision and Random Forrest - Next article will be followed with Gradient Boost (with more details) and the famous XGBOOST.

# Sources
* https://www.cse.ust.hk/~twinsen/Decision_Tree.ppt
* https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm
* http://www.cs.ucsb.edu/~ambuj/Courses/165B/Lectures/ensemble%20learning.ppt
* https://www.edureka.co/community/46102/disadvantages-of-using-decision-tree
* https://perso.math.univ-toulouse.fr/motimo/files/2013/07/random-forest.pdf
* https://www.datasciencecentral.com/profiles/blogs/decision-tree-vs-random-forest-vs-boosted-trees-explained
* https://www.youtube.com/watch?v=eKD5gxPPeY0
* https://spark.apache.org/docs/latest/ml-classification-regression.html