<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Debaditya Chakravorty">
    <meta name="description" content="About Word2Vec algorithm  Word2Vec is not a single algorithm Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space.  Word2Vec can utilize either of 2 model architectures –  CBOW (Continuous Bag of Words) Skip Gram    CBOW (Continuous Bag of Words) The input to the model could be wi−2,wi−1,wi&#43;1,wi&#43;2wi−2,wi−1,wi&#43;1,wi&#43;2, the preceding and following words of the current word we are at.">
    <meta name="keywords" content="blog,developer,personal">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word2Vec Algorithm"/>
<meta name="twitter:description" content="About Word2Vec algorithm  Word2Vec is not a single algorithm Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space.  Word2Vec can utilize either of 2 model architectures –  CBOW (Continuous Bag of Words) Skip Gram    CBOW (Continuous Bag of Words) The input to the model could be wi−2,wi−1,wi&#43;1,wi&#43;2wi−2,wi−1,wi&#43;1,wi&#43;2, the preceding and following words of the current word we are at."/>

    <meta property="og:title" content="Word2Vec Algorithm" />
<meta property="og:description" content="About Word2Vec algorithm  Word2Vec is not a single algorithm Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space.  Word2Vec can utilize either of 2 model architectures –  CBOW (Continuous Bag of Words) Skip Gram    CBOW (Continuous Bag of Words) The input to the model could be wi−2,wi−1,wi&#43;1,wi&#43;2wi−2,wi−1,wi&#43;1,wi&#43;2, the preceding and following words of the current word we are at." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/posts/word2vec/" />
<meta property="article:published_time" content="2020-07-11T20:24:10-04:00" />
<meta property="article:modified_time" content="2020-07-11T20:24:10-04:00" />


    
      <base href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/posts/word2vec/">
    
    <title>
  Word2Vec Algorithm · Data &amp; Tech Vidyārthī
</title>

    
      <link rel="canonical" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/posts/word2vec/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/css/coder.min.3219ef62ae52679b7a9c19043171c3cd9f523628c2a65f3ef247ee18836bc90b.css" integrity="sha256-MhnvYq5SZ5t6nBkEMXHDzZ9SNijCpl8&#43;8kfuGINryQs=" crossorigin="anonymous" media="screen" />
    

    

    

    

    

    <link rel="icon" type="image/png" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.73.0" />
  </head>

  
  
  <body class="colorscheme-light"
        onload=" twemoji.parse(document.body); "
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/">
      Data &amp; Tech Vidyārthī
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/">Home</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/concepts/">System-Design</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/posts/">AI &amp; ML</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/progs/">Programming</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/projects/">Projects</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">Word2Vec Algorithm</h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fas fa-calendar"></i>
              <time datetime='2020-07-11T20:24:10-04:00'>
                July 11, 2020
              </time>
            </span>
            <span class="reading-time">
              <i class="fas fa-clock"></i>
              1-minute read
            </span>
          </div>
          
          
        </div>
      </header>

      <div>
        
        <h1 id="about-word2vec-algorithm">About Word2Vec algorithm</h1>
<ul>
<li>Word2Vec is not a single algorithm</li>
<li>Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words.</li>
<li>Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space. </li>
<li>Word2Vec can utilize either of 2 model architectures –
<ul>
<li>CBOW (Continuous Bag of Words)</li>
<li>Skip Gram</li>
</ul>
</li>
</ul>
<h1 id="cbow-continuous-bag-of-words">CBOW (Continuous Bag of Words)</h1>
<p>The input to the model could be wi−2,wi−1,wi+1,wi+2wi−2,wi−1,wi+1,wi+2, the preceding and following words of the current word we are at. The output of the neural network will be wiwi. Hence you can think of the task as &ldquo;predicting the word given its context&rdquo;
<figure>
    <img src="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/cbow.JPG"/> 
</figure>
</p>
<h1 id="skip-grams-algorithm">Skip-Grams Algorithm</h1>
<p>The input to the model is wiwi, and the output could be wi−1,wi−2,wi+1,wi+2wi−1,wi−2,wi+1,wi+2. So the task here is &ldquo;predicting the context given a word&rdquo;. Also, the context is not limited to its immediate context, training instances can be created by skipping a constant number of words in its context, so for example, wi−3,wi−4,wi+3,wi+4wi−3,wi−4,wi+3,wi+4, hence the name skip-gram.
<figure>
    <img src="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/skipgram.JPG"/> 
</figure>
</p>
<h1 id="use-case--predicting-context-of-search-word-from-solr-documents-using-word2vec">Use-case : Predicting context of search word from SOLR Documents using Word2Vec</h1>
<figure>
    <img src="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/word2vecdesign.JPG"/> 
</figure>


      </div>


      <footer>
        


        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "debaditya" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Debaditya Chakravorty 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>

    </main>

    

    

    

  </body>

</html>
