<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">

    <meta name="author" content="Debaditya Chakravorty">
    <meta name="description" content="About Word2Vec algorithm  Word2Vec is not a single algorithm Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space.  Word2Vec can utilize either of 2 model architectures –  CBOW (Continuous Bag of Words) Skip Gram    CBOW (Continuous Bag of Words) The input to the model could be wi−2,wi−1,wi&#43;1,wi&#43;2wi−2,wi−1,wi&#43;1,wi&#43;2, the preceding and following words of the current word we are at.">
    <meta name="keywords" content="blog,developer,personal">

    
      <script src="https://twemoji.maxcdn.com/v/latest/twemoji.min.js" crossorigin="anonymous"></script>
    

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Word2Vec Algorithm"/>
<meta name="twitter:description" content="About Word2Vec algorithm  Word2Vec is not a single algorithm Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space.  Word2Vec can utilize either of 2 model architectures –  CBOW (Continuous Bag of Words) Skip Gram    CBOW (Continuous Bag of Words) The input to the model could be wi−2,wi−1,wi&#43;1,wi&#43;2wi−2,wi−1,wi&#43;1,wi&#43;2, the preceding and following words of the current word we are at."/>

    <meta property="og:title" content="Word2Vec Algorithm" />
<meta property="og:description" content="About Word2Vec algorithm  Word2Vec is not a single algorithm Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words. Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space.  Word2Vec can utilize either of 2 model architectures –  CBOW (Continuous Bag of Words) Skip Gram    CBOW (Continuous Bag of Words) The input to the model could be wi−2,wi−1,wi&#43;1,wi&#43;2wi−2,wi−1,wi&#43;1,wi&#43;2, the preceding and following words of the current word we are at." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/ai-ml-posts/word2vec/" />
<meta property="article:published_time" content="2020-07-11T20:24:10-04:00" />
<meta property="article:modified_time" content="2020-07-11T20:24:10-04:00" />


    
      <base href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/ai-ml-posts/word2vec/">
    
    <title>
  Word2Vec Algorithm · Debaditya&#39;s Tech Journal
</title>

    
      <link rel="canonical" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/ai-ml-posts/word2vec/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.13.0/css/all.css" integrity="sha384-Bfad6CLCknfcloXFOyFnlgtENryhrpZCe29RTifKEixXQZ38WheV+i/6YWSzkz3V" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/css/coder.min.3219ef62ae52679b7a9c19043171c3cd9f523628c2a65f3ef247ee18836bc90b.css" integrity="sha256-MhnvYq5SZ5t6nBkEMXHDzZ9SNijCpl8&#43;8kfuGINryQs=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/css/coder-dark.min.e78e80fc3a585a4d1c8fc7f58623b6ff852411e38431a9cd1792877ecaa160f6.css" integrity="sha256-546A/DpYWk0cj8f1hiO2/4UkEeOEManNF5KHfsqhYPY=" crossorigin="anonymous" media="screen" />
      
    

    

    

    <link rel="icon" type="image/png" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.73.0" />
  </head>

  
  
    
  
  <body class="colorscheme-dark"
        onload=" twemoji.parse(document.body); "
  >
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/">
      Debaditya&#39;s Tech Journal
    </a>
    
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/">Home</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/system-design-post/">System-Design</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/ai-ml-posts/">AI &amp; ML</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/code-concept-post/">Programming</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/projects/">Projects</a>
          </li>
        
      
      
    </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Word2Vec Algorithm</h1>
    </header>

    <h1 id="about-word2vec-algorithm">About Word2Vec algorithm</h1>
<ul>
<li>Word2Vec is not a single algorithm</li>
<li>Word2vec is a group of related models that are used to produce word embedding&rsquo;s. These models are two-layer neural networks that are trained to reconstruct linguistic contexts of words.</li>
<li>Word2vec takes large corpus of text as its input and produces a vector-space, with each unique word in the corpus being assigned a corresponding vector in the space. </li>
<li>Word2Vec can utilize either of 2 model architectures –
<ul>
<li>CBOW (Continuous Bag of Words)</li>
<li>Skip Gram</li>
</ul>
</li>
</ul>
<h1 id="cbow-continuous-bag-of-words">CBOW (Continuous Bag of Words)</h1>
<p>The input to the model could be wi−2,wi−1,wi+1,wi+2wi−2,wi−1,wi+1,wi+2, the preceding and following words of the current word we are at. The output of the neural network will be wiwi. Hence you can think of the task as &ldquo;predicting the word given its context&rdquo;
<figure>
    <img src="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/cbow.JPG"/> 
</figure>
</p>
<h1 id="skip-grams-algorithm">Skip-Grams Algorithm</h1>
<p>The input to the model is wiwi, and the output could be wi−1,wi−2,wi+1,wi+2wi−1,wi−2,wi+1,wi+2. So the task here is &ldquo;predicting the context given a word&rdquo;. Also, the context is not limited to its immediate context, training instances can be created by skipping a constant number of words in its context, so for example, wi−3,wi−4,wi+3,wi+4wi−3,wi−4,wi+3,wi+4, hence the name skip-gram.
<figure>
    <img src="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/skipgram.JPG"/> 
</figure>
</p>
<h1 id="use-case--predicting-context-of-search-word-from-solr-documents-using-word2vec">Use-case : Predicting context of search word from SOLR Documents using Word2Vec</h1>
<figure>
    <img src="http://techvidyarthi.com.s3-website.us-east-1.amazonaws.com/images/word2vecdesign.JPG"/> 
</figure>


  </article>
</section>

  

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
        2020
         Debaditya Chakravorty 
      
      
         · 
        Powered by <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
      
      
    </section>
  </footer>

    </main>

    

    

    

  </body>

</html>
